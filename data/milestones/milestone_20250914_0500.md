# Pool Scout Pro - Complete System Milestone Documentation

## System Overview

Pool Scout Pro is an AI-enhanced inspection report management system for Sacramento County pool inspections. The system searches EMD (Environmental Management Department) records, downloads PDF inspection reports, extracts data, and stores it in a normalized database with AI-generated violation summaries and intelligent caching.

**Core Philosophy**: Download → Extract → AI Summarize → Database (transactional flow ensures data consistency)

**Current System Status**: Enterprise-ready system with comprehensive resilience architecture and AI-powered violation summarization. All core systems operational with intelligent failure recovery, health monitoring, automatic retry mechanisms, separated database architecture for scalability, multi-session Selenium configuration for concurrent operations, comprehensive equipment tracking with violation deadline extraction, and AI-generated clean violation summaries.

---

## CRITICAL DEVELOPMENT RULES

**!! MANDATORY FOR ALL FUTURE AI ASSISTANTS !!**

**Any AI assistant MUST acknowledge these rules before making ANY code changes:**

### Development Rules - MANDATORY ADHERENCE
1. **ALWAYS GET PERMISSION BEFORE GENERATING CODE** - Never auto-generate code without explicit permission
2. **USE INTEGRATED DEVELOPMENT SYSTEM** - MANDATORY dev-tools workflow for ALL changes
3. **NEVER use sed, manual editing, or direct file modification** - All changes via dev-workflow.sh
4. **INCREMENTAL DEVELOPMENT** - One step at a time, validate each change
5. **COPY-PASTE READY COMMANDS** - All commands must run from project root
6. **NEVER MODIFY FILES WITHOUT SEEING THEM FIRST** - Always examine current content
7. **CLEAR STEP DELINEATION** - Structure commands with clear step comments
8. **ALWAYS VALIDATE SYNTAX** - Use dev-workflow.sh validate before any changes
9. **NEVER USE TEST/SAMPLE DATA IN PRODUCTION** - Use real data only, or work in separate environment
10. **Keep responses concise** - Avoid detailed explanations unless requested

**Required Acknowledgment**: "I will follow the mandatory dev-tools workflow for all changes"

**VIOLATION OF THESE RULES = IMMEDIATE CORRECTION REQUIRED**

# COMPLETE WORKFLOW: Create → Validate → Apply → Test → Cleanup

# Step 1: Create temporary file with new content
cat > temp_new_file.js << 'EOF'
[paste your new file content here]
EOF

# Step 2: Validate syntax before applying
./dev-tools/dev-workflow.sh validate temp_new_file.js

# Step 3: Apply changes via safe replacement
./dev-tools/dev-workflow.sh edit <target_file> temp_new_file.js

# Step 4: Test changes immediately
./dev-tools/dev-workflow.sh restart

# Step 5: Clean up temporary file after success
rm temp_new_file.js

# ALTERNATIVE: For git-style patches on large files
cat > temp_changes.patch << 'EOF'
[paste your patch content here]
EOF
./dev-tools/dev-workflow.sh validate <target_file>
./dev-tools/dev-workflow.sh patch <target_file> temp_changes.patch
./dev-tools/dev-workflow.sh restart
rm temp_changes.patch

# EMERGENCY: Rollback if issues detected
./dev-tools/dev-workflow.sh rollback <file>

# VALIDATION ONLY: Check syntax without changes
./dev-tools/dev-workflow.sh validate <existing_file>

# Replace FILENAME and paste content between EOF markers
cat > temp_FILENAME << 'EOF'
[CONTENT_HERE]
EOF
./dev-tools/dev-workflow.sh validate temp_FILENAME
./dev-tools/dev-workflow.sh edit path/to/target/FILENAME temp_FILENAME
./dev-tools/dev-workflow.sh restart
rm temp_FILENAME

---

## Complete Project Structure & File Purposes

```
pool_scout_pro/
├── config/
│   ├── __init__.py                     # Configuration package initialization
│   ├── logging.py                      # Logging configuration module
│   ├── logging_config.yaml             # YAML logging configuration
│   └── settings.yaml                   # Application settings configuration
├── data/
│   ├── backups/                        # Automatic timestamped file backups (dev-tools)
│   ├── inspection_data.db              # Business data SQLite database
│   ├── system_management.db            # Operational data SQLite database
│   ├── milestones/                     # Historical milestone documents
│   └── source_code/                    # Code snapshots for reference
├── dev-tools/                          # MANDATORY DEVELOPMENT SYSTEM
│   ├── dev-workflow.sh                 # Main workflow coordinator - PRIMARY INTERFACE
│   ├── validate-syntax.sh              # Python syntax validation with import testing
│   ├── safe-edit.sh                    # Safe file replacement with automatic backups
│   ├── safe-patch.sh                   # Git-style patch application with rollback
│   ├── rollback.sh                     # Quick restore from timestamped backups
│   ├── setup-git-workflow.sh           # Git branch setup for development
│   ├── workflow.sh                     # Testing and deployment coordination
│   └── run_extraction_test.py          # PDF extraction testing utility
├── schemas/
│   └── inspection_schema.sql           # Complete business database schema
├── scripts/
│   ├── check_selenium.sh               # Selenium container health check
│   ├── maintain_project.sh             # Project maintenance utilities
│   ├── migrate_addresses.py            # Address data migration script
│   ├── monitor_selenium.sh             # Selenium monitoring script
│   ├── populate_database.py            # Database population utility
│   ├── rebuild_equipment_from_reports.py # Equipment data reconstruction
│   └── reprocess_all.py                # Bulk AI summarization processing utility
├── src/
│   ├── __init__.py                     # Source package initialization
│   ├── core/                           # Core system utilities
│   │   ├── __init__.py                 # Core package initialization
│   │   ├── browser.py                  # Browser automation utilities
│   │   ├── database.py                 # Database connection management
│   │   ├── database_config.py          # Database configuration
│   │   ├── error_handler.py            # Error handling utilities
│   │   ├── settings.py                 # Application settings management
│   │   └── utilities.py                # General utility functions
│   ├── services/                       # Business logic services
│   │   ├── __init__.py                 # Services package initialization
│   │   ├── database_service.py         # Database operations service
│   │   ├── download_lock_service.py    # Download concurrency management
│   │   ├── download_progress_service.py # Download progress tracking
│   │   ├── duplicate_prevention_service.py # Duplicate download prevention
│   │   ├── failed_download_service.py  # Failed download tracking
│   │   ├── pdf_downloader.py           # PDF download service with enterprise resilience
│   │   ├── pdf_extractor.py            # PDF content extraction with equipment tracking
│   │   ├── retry_service.py            # Retry queue processing service
│   │   ├── robust_selenium_monitor.py  # Application-level Selenium monitoring
│   │   ├── saved_status_service.py     # Report saved status management
│   │   ├── search_progress_service.py  # Search progress tracking
│   │   ├── search_service.py           # EMD website search operations
│   │   ├── selenium_health_service.py  # Selenium health verification
│   │   ├── violation_severity_service.py # Violation severity classification
│   │   └── violation_summarizer.py     # AI-powered violation summarization service
│   └── web/                            # Web application layer
│       ├── __init__.py                 # Web package initialization
│       ├── app.py                      # Main Flask application
│       ├── routes/                     # Web route handlers
│       │   ├── __init__.py             # Routes package initialization
│       │   ├── api_routes.py           # API endpoint handlers
│       │   └── downloads.py            # Download operation endpoints
│       └── shared/                     # Shared web utilities
│           ├── __init__.py             # Shared package initialization
│           └── services.py             # Shared service utilities
├── templates/                          # Web interface templates and assets
│   ├── search_reports.html             # Main search interface template
│   └── static/                         # Frontend static assets
│       ├── css/
│       │   └── search_reports.css      # Application styling
│       └── js/                         # Frontend JavaScript modules
│           ├── main.js                 # Main application entry point
│           ├── vendor/
│           │   └── lucide.js           # Icon library
│           ├── core/                   # Core frontend modules
│           │   ├── api-client.js       # API communication layer
│           │   ├── page-state-service.js # Application state management
│           │   ├── report-modal.js     # Enhanced report detail modal
│           │   ├── ui-manager.js       # UI interaction management
│           │   ├── utilities.js        # Frontend utility functions
│           │   └── violation-modal.js  # Violation details popup
│           ├── downloads/              # Download functionality modules
│           │   ├── download-poller.js  # Real-time progress polling
│           │   ├── download-service.js # Download operations service
│           │   └── download-ui.js      # Download UI management
│           └── search/                 # Search functionality modules
│               ├── search-service.js   # Search operations service
│               └── search-ui.js        # Search UI management
├── docker-compose.yml                  # Docker services configuration
├── gunicorn.conf.py                    # Gunicorn WSGI server configuration
├── health_monitor.sh                   # System health monitoring script
├── manage.sh                          # Enterprise management script
├── README.md                          # Project documentation
├── requirements.txt                    # Python dependencies
├── run.py                             # Development server entry point
├── test_debug.html                    # Debug testing template
└── wsgi.py                            # Production WSGI entry point
```

### External System Files
```
/etc/systemd/system/
├── pool-scout-pro.service             # Main application systemd service
├── pool-scout-health.service          # Health monitoring service
├── pool-scout-health.timer            # Health check timer (every 2 minutes)
├── pool-scout-retry.service           # Retry processing service
└── pool-scout-retry.timer             # Retry timer (every 15 minutes)

/usr/local/bin/
├── selenium_monitor.sh                # External Selenium health monitor
└── retry_job.sh                       # Scheduled retry processor

/mnt/nas-pool-data/reports/2025/       # PDF file storage location
```

---

## Database Architecture

### Business Database (inspection_data.db)

**Complete Schema:**
```sql
CREATE TABLE facilities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL,
    street_address TEXT,
    city TEXT,
    state TEXT,
    zip_code TEXT,
    phone TEXT,
    facility_id TEXT,
    permit_holder TEXT,
    program_identifier TEXT,
    facility_type TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE inspection_reports (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    facility_id INTEGER NOT NULL,
    permit_id TEXT,
    inspection_date TEXT,
    inspection_type TEXT,
    inspector_name TEXT,
    inspector_phone TEXT,
    report_recipient TEXT,
    total_violations INTEGER DEFAULT 0,
    major_violations INTEGER DEFAULT 0,
    pdf_filename TEXT,
    pdf_path TEXT,
    report_notes TEXT,
    co_inspector TEXT,
    reinspection_required BOOLEAN DEFAULT FALSE,
    closure_suspension_required BOOLEAN DEFAULT FALSE,
    barcode_data TEXT,
    pool_capacity_gallons INTEGER,
    pool_flow_rate_gpm INTEGER,
    water_chemistry_details TEXT,
    closure_status TEXT DEFAULT 'operational',
    closure_reason TEXT,
    inspection_id TEXT UNIQUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (facility_id) REFERENCES facilities(id)
);

CREATE TABLE violations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    report_id INTEGER NOT NULL,
    facility_id INTEGER NOT NULL,
    violation_code TEXT,
    violation_title TEXT,
    observations TEXT,
    corrective_action TEXT,
    code_description TEXT,
    is_major_violation BOOLEAN DEFAULT FALSE,
    severity_level INTEGER,
    correction_deadline_days INTEGER,
    shorthand_summary TEXT,
    FOREIGN KEY (report_id) REFERENCES inspection_reports(id),
    FOREIGN KEY (facility_id) REFERENCES facilities(id)
);

CREATE TABLE equipment (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    report_id INTEGER NOT NULL,
    facility_id INTEGER NOT NULL,
    pool_capacity_gallons INTEGER,
    flow_rate_gpm INTEGER,
    filter_pump_1_make TEXT, filter_pump_1_model TEXT, filter_pump_1_hp REAL,
    filter_pump_2_make TEXT, filter_pump_2_model TEXT, filter_pump_2_hp REAL,
    filter_pump_3_make TEXT, filter_pump_3_model TEXT, filter_pump_3_hp REAL,
    jet_pump_1_make TEXT, jet_pump_1_model TEXT, jet_pump_1_hp REAL,
    jet_pump_2_make TEXT, jet_pump_2_model TEXT, jet_pump_2_hp REAL,
    filter_1_type TEXT, filter_1_make TEXT, filter_1_model TEXT, filter_1_capacity_gpm INTEGER,
    filter_2_type TEXT, filter_2_make TEXT, filter_2_model TEXT, filter_2_capacity_gpm INTEGER,
    sanitizer_1_type TEXT, sanitizer_1_details TEXT,
    sanitizer_2_type TEXT, sanitizer_2_details TEXT,
    main_drain_type TEXT, main_drain_model TEXT, main_drain_install_date TEXT,
    equalizer_model TEXT, equalizer_install_date TEXT,
    equipment_matches_emd BOOLEAN DEFAULT TRUE,
    filter_notes TEXT, pump_notes TEXT, jet_pump_notes TEXT,
    sanitizer_notes TEXT, main_drain_notes TEXT, equalizer_notes TEXT, equipment_notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (report_id) REFERENCES inspection_reports(id),
    FOREIGN KEY (facility_id) REFERENCES facilities(id)
);

CREATE TABLE violation_summary_cache (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    fingerprint TEXT NOT NULL UNIQUE,
    original_title TEXT NOT NULL,
    original_observations TEXT NOT NULL,
    shorthand_summary TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### System Management Database (system_management.db)

**Complete Schema:**
```sql
CREATE TABLE failed_downloads (
    id INTEGER PRIMARY KEY,
    facility_name TEXT NOT NULL,
    inspection_id TEXT,
    pdf_url TEXT NOT NULL,
    inspection_date TEXT,
    failure_reason TEXT,
    failure_details TEXT,
    retry_count INTEGER DEFAULT 0,
    max_retries INTEGER DEFAULT 2,
    next_retry_at TIMESTAMP,
    original_batch_id TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_retry_at TIMESTAMP,
    status TEXT DEFAULT 'pending'
);

CREATE TABLE search_timing (
    id INTEGER PRIMARY KEY,
    search_date TEXT NOT NULL,
    duration_seconds REAL NOT NULL,
    facility_count INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE retry_queue (
    id INTEGER PRIMARY KEY,
    failed_download_id INTEGER REFERENCES failed_downloads(id),
    retry_count INTEGER DEFAULT 0,
    next_retry_at TEXT,
    status TEXT DEFAULT 'PENDING',
    last_attempt_at TEXT,
    created_at TEXT DEFAULT (datetime('now')),
    updated_at TEXT DEFAULT (datetime('now'))
);

CREATE TABLE health_status (
    id INTEGER PRIMARY KEY,
    component TEXT,
    status TEXT,
    last_check TEXT DEFAULT (datetime('now')),
    failure_count INTEGER DEFAULT 0,
    details TEXT,
    updated_at TEXT DEFAULT (datetime('now'))
);

CREATE TABLE search_timings (
    id INTEGER PRIMARY KEY,
    search_date TEXT NOT NULL,
    duration REAL NOT NULL,
    timestamp TIMESTAMP NOT NULL
);

-- Indexes for Performance
CREATE INDEX idx_retry_queue_status_next_retry ON retry_queue(status, next_retry_at);
CREATE INDEX idx_health_status_component ON health_status(component, last_check);
CREATE INDEX idx_search_timing_date ON search_timing(search_date);
CREATE INDEX idx_failed_downloads_batch ON failed_downloads(original_batch_id, created_at);
CREATE INDEX idx_search_timings_date ON search_timings(search_date);
CREATE INDEX idx_failed_downloads_status_created ON failed_downloads(status, created_at);
CREATE INDEX idx_failed_downloads_retry_count ON failed_downloads(retry_count, next_retry_at);
CREATE INDEX idx_health_status_component_check ON health_status(component, last_check DESC);

-- Retry Candidates View
CREATE VIEW retry_candidates AS
SELECT fd.*
FROM failed_downloads fd
WHERE fd.status = 'pending'
  AND fd.retry_count < fd.max_retries
  AND (fd.next_retry_at IS NULL OR datetime(fd.next_retry_at) <= datetime('now'))
ORDER BY fd.created_at;
```

---

## System Architecture & Capabilities

### Separation of Operations
The system maintains strict separation between:

- **Search Operations** (SearchService) - EMD website interaction and facility discovery
- **Download Operations** (PDFDownloader) - PDF file acquisition and transactional processing with enterprise resilience
- **Extraction Operations** (PDFExtractor) - PDF content parsing with comprehensive equipment tracking and violation deadline extraction
- **AI Summarization** (ViolationSummarizer) - AI-powered violation summary generation with intelligent caching
- **Database Operations** (DatabaseService) - Data persistence and integrity with enterprise architecture
- **Web Interface** (Flask) - User interaction and API endpoints with enhanced report modals
- **Health Monitoring** (SeleniumHealthService) - Container health verification and recovery
- **External Monitoring** - Independent Selenium monitoring with restart capability and alerting
- **Retry Management** - Dead letter queue processing and scheduled retry operations
- **Multi-Session Management** - Concurrent session handling with intelligent resource management

### AI-Powered Violation Summarization System

**AI Integration Architecture:**
- **Local LLM Integration**: Ollama container with llama3 model
- **Intelligent Caching**: Violation code-based fingerprinting prevents redundant AI calls
- **Generic Summaries**: Creates standardized summaries based on violation types, not specific details
- **High Cache Hit Rate**: Efficient system processes 1,386+ reports with minimal AI calls

**Caching Strategy:**
- **Cache Key**: Uses violation_code (e.g., "12a", "24") as unique identifier
- **Cache Storage**: violation_summary_cache table with fingerprint-based lookups
- **Cache Logic**: Check cache → Use existing OR call AI → Save new summary
- **Performance**: Dramatically reduces AI processing time for repeat violation types

**AI Model Configuration:**
- **Model**: llama3 running in Docker container
- **Endpoint**: http://localhost:11434/api/generate
- **Temperature**: 0.0 for consistent outputs
- **Prompt Engineering**: Generates concise summaries (e.g., "Low pH Level" vs full inspector text)

### Multi-Session Selenium Architecture

**Purpose**: Eliminate single session bottleneck and enable concurrent PDF downloads

**Configuration**: 5 concurrent sessions (increased from 1) with resource management

**Benefits**: 5x Download Speed, Fault Tolerance, Resource Optimization, Graceful Degradation

**Session Management**: Automatic cleanup of idle and stuck sessions with configurable thresholds

**Health Monitoring**: Per-session health tracking with utilization monitoring

### Enterprise Database Architecture

**Purpose**: Separate business data from operational data for scalability and data lifecycle management

**Implementation**: Two-database architecture with clear separation of concerns

**Databases**:
- inspection_data.db (business data)
- system_management.db (operational data)

**Benefits**: Independent backup strategies, different retention policies, scalable to multiple counties

---

## Infrastructure Configuration

### Docker Services Configuration

**docker-compose.yml Services:**
```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - /mnt/nas/Projects/ollama:/root/.ollama
    networks:
      - ai-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 12
```

### Systemd Services Configuration

**Main Application Service:**
- **Service**: pool-scout-pro.service
- **Type**: Gunicorn WSGI server
- **User**: brian
- **Working Directory**: /home/brian/sapphire/pool_scout_pro
- **Environment**: Virtual environment with PYTHONPATH configuration
- **Security**: Hardened with ProtectSystem, PrivateTmp, NoNewPrivileges

**Health Monitoring:**
- **Timer**: pool-scout-health.timer (every 2 minutes)
- **Service**: pool-scout-health.service
- **Script**: /home/brian/sapphire/pool_scout_pro/health_monitor.sh

**Retry Processing:**
- **Timer**: pool-scout-retry.timer (every 15 minutes)
- **Service**: pool-scout-retry.service
- **Script**: Python retry service with 5-minute timeout

### External Monitoring Scripts

**Selenium Monitor**: `/usr/local/bin/selenium_monitor.sh`
- Container health verification
- Automatic restart capability
- Alert logging to `/var/log/selenium_monitor.log`

**Retry Processor**: `/usr/local/bin/retry_job.sh`
- Scheduled retry queue processing
- Application availability verification
- Logging to `/var/log/pool_scout_retry.log`

---

## API Endpoints & Web Interface

### API Endpoints

**Search Operations:**
- `POST /api/v1/reports/search-with-duplicates` - EMD search with duplicate detection
- `POST /api/v1/reports/existing-for-date` - Get saved reports for date with AI summaries

**Download Operations:**
- `POST /api/v1/downloads/start` - Initialize download process
- `GET /api/v1/downloads/progress` - Real-time download progress

### Web Interface Components

**Frontend Architecture:**
- **Framework**: Vanilla JavaScript with modular architecture
- **Main Template**: templates/search_reports.html
- **Styling**: templates/static/css/search_reports.css
- **State Management**: page-state-service.js
- **API Communication**: api-client.js

**Enhanced Features:**
- **Clean Violation Display**: Shows top 2 AI-generated summaries per report
- **Detailed Report Modal**: Comprehensive pop-out with inspection details, violations, and notes
- **Improved Table Layout**: Optimized column widths for better readability
- **Interactive Elements**: Clickable facility names and findings with modal displays

---

## Development & Maintenance Processes

### Integrated Development System

**Safe Development Workflow:**
1. **Git Branch Strategy**: All changes on development branch
2. **Pre-flight Validation**: Syntax check before ANY file modification
3. **Automatic Backups**: Every change creates timestamped backup in `data/backups/`
4. **Atomic Operations**: Backup → Validate → Apply → Verify → Success/Rollback
5. **Zero-Downtime Recovery**: Instant rollback if validation fails
6. **Service Integration**: Direct testing via service restart commands

**Development Toolkit (dev-tools/):**
- **dev-workflow.sh**: Main workflow coordinator - PRIMARY INTERFACE
- **validate-syntax.sh**: Python syntax validation with import testing
- **safe-edit.sh**: Safe file replacement with automatic backups
- **safe-patch.sh**: Git-style patch application with rollback
- **rollback.sh**: Quick restore from timestamped backups
- **setup-git-workflow.sh**: Git branch setup for development
- **workflow.sh**: Testing and deployment coordination

**Safety Features:**
- Pre-flight Validation: Python compilation + import testing before application
- Automatic Backups: Timestamped backups prevent data loss
- Rollback on Failure: Auto-restore if post-application validation fails
- Import Testing: Ensures modified Python modules can be imported correctly
- Service Integration: Immediate testing via manage.sh integration
- Git Integration: Development branch isolation from production code

### Backup & Recovery Processes

**Automatic File Backups:**
- **Location**: `data/backups/`
- **Format**: `filename.YYYYMMDD_HHMMSS`
- **Trigger**: Every file modification via dev-tools
- **Retention**: Manual cleanup required

**Database Backup Strategy:**
- **Business Data**: Regular sqlite3 dumps of inspection_data.db
- **Operational Data**: System management database backups
- **Separation**: Independent backup schedules for different data types

### Maintenance Procedures

**Health Monitoring:**
- **Application Health**: health_monitor.sh via systemd timer
- **Container Health**: selenium_monitor.sh external monitoring
- **Database Health**: Connection verification and query performance
- **Retry Queue**: Automated processing via systemd timer

**Log Management:**
- **Application Logs**: /var/log/pool_scout_pro/
- **System Logs**: journald integration
- **Monitoring Logs**: /var/log/selenium_monitor.log, /var/log/pool_scout_retry.log

**Performance Monitoring:**
- **Search Timing**: Tracked in search_timing table
- **Download Progress**: Real-time tracking via web interface
- **Session Utilization**: Multi-session Selenium monitoring

---

## Deployment Procedures

### Production Deployment
```bash
# Start Docker services
docker-compose up -d

# Start main application
./manage.sh start
```

### Health Verification
```bash
# Check application status
./manage.sh status

# Verify Selenium container
curl -f http://localhost:4444/wd/hub/status

# Verify AI service
curl -f http://localhost:11434/api/tags

# Check systemd timers
sudo systemctl list-timers | grep pool-scout
```

### Database Initialization
```bash
# Initialize business database
sqlite3 data/inspection_data.db < schemas/inspection_schema.sql

# Initialize system database (schema file needs to be created)
sqlite3 data/system_management.db < schemas/system_schema.sql
```

### AI Model Setup
```bash
# Download and verify AI model
docker exec -it ollama ollama pull llama3
docker exec -it ollama ollama list  # Verify model installation
```

### Bulk AI Processing
```bash
# Process historical data with AI summaries
python3 scripts/reprocess_all.py  # Process historical data
```

---

## Current System Status

### Operational Capabilities

**All Core Systems: FULLY OPERATIONAL**

✅ **EMD Website Search**: Enhanced logging and performance tracking
✅ **Intelligent PDF Download**: Multi-session concurrent downloads with enterprise resilience
✅ **Comprehensive PDF Extraction**: Equipment tracking and violation deadline extraction
✅ **AI-Powered Violation Summarization**: Local LLM with intelligent caching
✅ **Advanced Database Operations**: Two-database architecture with retry management
✅ **Enhanced Web Interface**: AI summaries, detailed report modals, dynamic UI state management
✅ **Enterprise Monitoring & Recovery**: Systemd timer-based health checks with automatic restart
✅ **Bulk Processing Tools**: Historical data enhancement and AI summary backfilling

## System & Network Configuration

### Hardware Environment
- **Host Server**: MS-01 Server (1TB SSD, 64GB RAM, AX2000 GPU)
- **IP Address**: 10.10.10.80/24 (static)
- **Operating System**: Ubuntu 24.04.3 LTS (Noble Numbat)
- **Kernel**: Linux 6.8.0-79-generic x86_64
- **Network**: Home network 10.10.10.0/24

### Network Infrastructure
- **Primary Interface**: enp88s0 (10.10.10.80/24)
- **NAS Storage**: Synology NAS at 10.10.10.70
- **Remote Access**: DynDNS with router port forwarding
  - sapphire-pools.dyndns.org (HTTPS/SSL)
  - besc-ai.dyndns.org (AI services)

### Docker Network Configuration
```yaml
Docker Networks:
- pool_scout_pro_ai-network (172.19.0.0/16)
- openwebui_ai-network (172.18.0.0/16)
- pool_scout_pro_default (172.21.0.0/16)
Storage Architecture
NFS Mounts:
- 10.10.10.70:/volume1/pool_scout_pro → /mnt/nas/pool_scout_pro (5.3TB, 5% used)
- 10.10.10.70:/volume1/ShareDrive → /mnt/nas/Projects (5.3TB, 5% used)

Storage Strategy:
- PDF Reports: /mnt/nas/pool_scout_pro/reports/ (NAS storage)
- AI Models: /mnt/nas/Projects/ollama (NAS storage)
- Database Files: Local SSD for performance
- Backups: NAS storage for durability
Service Ports & Reverse Proxy
Active Services:
- Port 80/443: Nginx reverse proxy (SSL termination)
- Port 4444: Selenium Grid
- Port 11434: Ollama AI service
- Port 5000: Flask development (internal)
- Port 7001: Pool Scout Pro (proxied)
- Port 7003: OpenWebUI (proxied)

Nginx Proxy Configuration:
- sapphire-pools.dyndns.org → localhost:7001 (Pool Scout Pro)
- besc-ai.dyndns.org → localhost:7003 (AI Interface)
- Local access: 10.10.10.80 → localhost:7001
Resource Optimization Strategy

Compute: MS-01 handles application logic and AI processing (GPU acceleration)
Storage: NAS handles all file storage (PDFs, models, backups)
Network: Local gigabit for NAS, external via DynDNS
Monitoring: Distributed between local systemd and external access

---

## MILESTONE MAINTENANCE RULES

**FOR FUTURE AI ASSISTANTS:**

This milestone contains ONLY:
✅ Project structure and file purposes
✅ Development rules and standards
✅ System capabilities and configuration
✅ Database schemas and architecture
✅ Deployment and maintenance procedures
✅ Current operational status

**DO NOT ADD:**
❌ Session-specific resolved issues
❌ Temporary status updates
❌ "Next phase priorities" or similar changing content
❌ Backup files, enhanced files, or dated suffixes in structure

**WHEN UPDATING THIS MILESTONE:**
1. **PRESERVE** all architectural information
2. **UPDATE** only permanent system capabilities or changes
3. **MAINTAIN** complete file structure and purposes
4. **VERIFY** all database schemas remain complete
5. **ENSURE** development rules are prominent and clear

This milestone serves as the definitive reference for Pool Scout Pro's complete system architecture and must be preserved in its comprehensive form.

---

**Last Updated**: September 14, 2025
**System Status**: All systems operational and enterprise-ready
